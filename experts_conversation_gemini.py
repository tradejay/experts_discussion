# app.py

import streamlit as st
import os
from groq import Groq
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from collections import Counter, deque
from typing import Dict, List, Set, Tuple, Optional
import google.generativeai as genai


# Constants
MAX_RETRIES = 3
RETRY_DELAY = 2
TOKEN_LIMIT = 7000
API_RATE_LIMIT = 30
API_TIME_WINDOW = 60

# ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì— logs í´ë” ìƒì„±
        current_dir = Path(__file__).parent
        log_dir = current_dir / 'logs'
        log_dir.mkdir(exist_ok=True)
        
        # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = log_dir / f'conversation_log_{timestamp}.log'
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        
        # ë¡œê¹… í¬ë§· ì„¤ì •
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()  # ì½˜ì†” ì¶œë ¥ìš©
            ]
        )
        
        logging.info("=== ë¡œê¹… ì‹œì‘ (UTF-8 ì¸ì½”ë”©) ===")
        logging.info(f"ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {log_file}")
        return True
        
    except Exception as e:
        print(f"ë¡œê¹… ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def log_conversation_state(state_name, content):
    """ëŒ€í™” ìƒíƒœë¥¼ ë¡œê·¸ì— ê¸°ë¡"""
    try:
        logging.info(f"\n=== {state_name} ===\n{json.dumps(content, ensure_ascii=False, indent=2)}")
    except Exception as e:
        logging.error(f"ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def load_api_key():
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(base_dir, 'gemini_api.txt')  # groq_api.txt -> gemini_api.txt
        with open(file_path, 'r') as file:
            return file.read().strip()
    except FileNotFoundError:
        return None


def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    try:
        if 'conversation_context' not in st.session_state:
            st.session_state.conversation_context = ConversationContext()
            logging.info("Conversation context initialized")
            
        if 'rate_limiter' not in st.session_state:
            st.session_state.rate_limiter = APIRateLimiter()
            logging.info("Rate limiter initialized")
            
        if 'expert_data' not in st.session_state:
            st.session_state.expert_data = []
            logging.info("Expert data initialized")
            
        if 'current_round' not in st.session_state:
            st.session_state.current_round = 0
            logging.info("Round counter initialized")
            
        if 'session_keywords' not in st.session_state:
            st.session_state.session_keywords = set()
            logging.info("Session keywords initialized")
            
        if 'conversation_history' not in st.session_state:
            st.session_state.conversation_history = []
            logging.info("Conversation history initialized")
            
        if 'user_profile' not in st.session_state:
            st.session_state.user_profile = load_user_profile()
            logging.info("User profile initialized")
        
        logging.info("Session state initialization completed")
        
    except Exception as e:
        error_msg = f"Session state initialization error: {str(e)}"
        logging.error(error_msg)
        raise Exception(error_msg)

def initialize_api_key():
    """Initialize or update API key from user input"""
    st.sidebar.markdown("## ğŸ”‘ API Configuration")
    
    # Session stateì— ì €ì¥ëœ API í‚¤ í™•ì¸
    if 'api_key' not in st.session_state:
        st.session_state.api_key = None
        
    # API í‚¤ ì…ë ¥ í•„ë“œ (password íƒ€ì…ìœ¼ë¡œ ë§ˆìŠ¤í‚¹)
    api_key_input = st.sidebar.text_input(
        "Enter Gemini API Key",
        type="password",
        placeholder="Enter your API key here...",
        value=st.session_state.api_key if st.session_state.api_key else "",
        help="Your API key will be stored only for this session"
    )
    
    # API í‚¤ ì ìš© ë²„íŠ¼
    if st.sidebar.button("Apply API Key"):
        if api_key_input:
            st.session_state.api_key = api_key_input
            try:
                # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
                genai.configure(api_key=api_key_input)
                test_client = genai.GenerativeModel('gemini-1.5-pro-latest')
                st.sidebar.success("âœ… API key successfully configured!")
                return True
            except Exception as e:
                st.sidebar.error(f"âŒ Invalid API key: {str(e)}")
                st.session_state.api_key = None
                return False
        else:
            st.sidebar.warning("âš ï¸ Please enter an API key")
            return False
    
    # ì´ë¯¸ ìœ íš¨í•œ API í‚¤ê°€ ìˆëŠ” ê²½ìš°
    return bool(st.session_state.api_key)


def analyze_previous_logs():
    """ì´ì „ ëŒ€í™” ë¡œê·¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±"""
    try:
        # ë¡œê·¸ íŒŒì¼ë“¤ ì°¾ê¸°
        base_dir = os.path.dirname(os.path.abspath(__file__))
        log_files = [f for f in os.listdir(base_dir) if f.startswith('conversation_log_') and f.endswith('.log')]
        
        if not log_files:
            return None
            
        # ìµœê·¼ 3ê°œì˜ ë¡œê·¸ íŒŒì¼ë§Œ ë¶„ì„
        recent_logs = sorted(log_files, reverse=True)[:3]
        user_interactions = []
        
        for log_file in recent_logs:
            with open(os.path.join(base_dir, log_file), 'r', encoding='utf-8') as f:
                content = f.read()
                # ì‚¬ìš©ì ì…ë ¥ê³¼ ëŒ€í™” íŒ¨í„´ ë¶„ì„
                user_interactions.extend([line for line in content.split('\n') if 'ì‚¬ìš©ì ì…ë ¥' in line])
        
        return analyze_user_profile(user_interactions)
    except Exception as e:
        logging.error(f"ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def analyze_user_profile(interactions):
    """ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ ë¶„ì„í•˜ì—¬ í”„ë¡œí•„ ìƒì„±"""
    if not interactions:
        return {
            "expertise_level": "intermediate",  # ê¸°ë³¸ê°’
            "interest_areas": [],
            "complexity_preference": 0.7
        }
    
    # ì‚¬ìš©ì ì…ë ¥ì˜ ë³µì¡ë„ì™€ ì „ë¬¸ì„± ë¶„ì„
    total_words = 0
    technical_terms = 0
    topics = []
    
    for interaction in interactions:
        try:
            content = json.loads(interaction.split("ì‚¬ìš©ì ì…ë ¥ ===")[-1].strip())
            user_input = content.get("user_todo", "")
            
            words = user_input.split()
            total_words += len(words)
            
            # ê¸°ìˆ  ìš©ì–´ë‚˜ ì „ë¬¸ ìš©ì–´ ì¹´ìš´íŠ¸ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            technical_terms += sum(1 for word in words if len(word) > 7)
            
            # ì£¼ì œ ì˜ì—­ ì¶”ì¶œ
            topics.extend([word for word in words if len(word) > 3])
        except:
            continue
    
    # ì „ë¬¸ì„± ìˆ˜ì¤€ ê³„ì‚°
    if total_words > 0:
        expertise_ratio = technical_terms / total_words
        if expertise_ratio > 0.3:
            level = "expert"
        elif expertise_ratio > 0.1:
            level = "intermediate"
        else:
            level = "beginner"
    else:
        level = "intermediate"
    
    return {
        "expertise_level": level,
        "interest_areas": list(set(topics))[:5],  # ìƒìœ„ 5ê°œ ê´€ì‹¬ ì˜ì—­
        "complexity_preference": min(0.9, 0.5 + expertise_ratio)
    }

def summarize_conversation(client: genai.GenerativeModel, conversation_history: List[Dict]) -> str:
    """ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½"""
    try:
        # ëŒ€í™” ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        conversation_text = ""
        for entry in conversation_history:
            conversation_text += f"\nExpert ({entry['expert']}): {entry['expert_response']}\n"
            if 'manager_feedback' in entry:
                conversation_text += f"Manager Feedback: {entry['manager_feedback']}\n"
            if 'keywords' in entry:
                conversation_text += f"Keywords: {', '.join(entry['keywords'])}\n"
        
        summary_prompt = f"""
        Please provide a comprehensive summary of the following expert discussion.
        Focus on:
        1. Main topics and key insights
        2. Different perspectives from experts
        3. Critical conclusions
        4. Areas of agreement and disagreement
        5. Key recommendations or action items

        Discussion Content:
        {conversation_text}

        Please structure the summary as follows:
        ğŸ¯ Main Topics:
        - [Key topics discussed]

        ğŸ’¡ Key Insights:
        - [Major insights and findings]

        ğŸ‘¥ Expert Perspectives:
        - [Different viewpoints]

        âœ¨ Conclusions:
        - [Main conclusions]

        ğŸ“‹ Recommendations:
        - [Action items or suggestions]
        """

        # Gemini ìƒì„± ì„¤ì •
        generation_config = {
            'temperature': 0.3,  # ë” ì‚¬ì‹¤ì ì¸ ìš”ì•½ì„ ìœ„í•´ ë‚®ì€ temperature
            'top_p': 0.8,
            'top_k': 40,
            'max_output_tokens': 2048,
        }

        # ìš”ì•½ ìƒì„±
        response = client.generate_content(
            summary_prompt,
            generation_config=generation_config
        )

        summary = response.text.strip()
        logging.info(f"Conversation summary generated successfully")
        return summary

    except Exception as e:
        error_msg = f"Error generating conversation summary: {str(e)}"
        logging.error(error_msg)
        return f"Failed to generate summary: {str(e)}"

def save_conversation_summary(summary: str, user_todo: str) -> str:
    """Save conversation summary to a file"""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì— summaries í´ë” ìƒì„±
        current_dir = Path(__file__).parent
        summary_dir = current_dir / 'summaries'
        summary_dir.mkdir(exist_ok=True)
        
        # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        # ì£¼ì œë¥¼ íŒŒì¼ëª…ì— í¬í•¨ (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ)
        topic = "".join(c for c in user_todo[:30] if c.isalnum() or c.isspace()).strip()
        topic = topic.replace(' ', '_')
        
        file_name = f"summary_{timestamp}_{topic}.md"
        file_path = summary_dir / file_name
        
        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# Discussion Summary\n\n")
            f.write(f"**Topic**: {user_todo}\n\n")
            f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## Summary Content\n\n")
            f.write(summary)
            
        logging.info(f"Summary saved to: {file_path}")
        return str(file_path)
        
    except Exception as e:
        error_msg = f"Error saving summary: {str(e)}"
        logging.error(error_msg)
        raise

def create_expert_team(client: genai.GenerativeModel, user_todo, user_profile=None, max_retries=3):
    """ì „ë¬¸ê°€ íŒ€ êµ¬ì„± í•¨ìˆ˜"""
    for attempt in range(max_retries):
        try:
            expert_team_prompt = create_expert_team_prompt(user_todo, user_profile)
            
            # ë¡œê¹… ì¶”ê°€
            logging.info(f"ì „ë¬¸ê°€ íŒ€ ìƒì„± í”„ë¡¬í”„íŠ¸ (ì‹œë„ {attempt + 1}):\n{expert_team_prompt}")
            
            # Gemini ìƒì„± ì„¤ì •
            generation_config = {
                'temperature': 0.7,
                'top_p': 0.8,
                'top_k': 40,
                'max_output_tokens': 2048,
                'candidate_count': 1,
            }
            
            # Gemini API í˜¸ì¶œ
            response = client.generate_content(
                expert_team_prompt,
                generation_config=generation_config
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = response.text.strip()
            
            # ì‘ë‹µ ë¡œê¹…
            logging.info(f"LLM ì‘ë‹µ (ì‹œë„ {attempt + 1}):\n{response_text}")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logging.warning(f"JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                continue
                
            json_content = response_text[json_start:json_end]
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                experts_config = json.loads(json_content)
            except json.JSONDecodeError as e:
                logging.error(f"JSON íŒŒì‹± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}\nì‘ë‹µ: {json_content}")
                continue
            
            if 'experts' not in experts_config:
                logging.warning(f"experts í‚¤ê°€ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                continue
            
            # ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦
            experts = experts_config['experts']
            all_valid = True
            error_message = ""
            
            for expert in experts:
                is_valid, error = validate_expert_data(expert)
                if not is_valid:
                    all_valid = False
                    error_message = error
                    logging.error(f"ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error}")
                    break
            
            if all_valid:
                logging.info("ì „ë¬¸ê°€ íŒ€ ìƒì„± ì„±ê³µ")
                return True, experts
            else:
                if attempt == max_retries - 1:
                    return False, f"ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_message}"
                continue
                
        except Exception as e:
            logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}")
            if attempt == max_retries - 1:
                return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            continue
    
    return False, "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

def format_conversation_history(history, max_entries=6):
    """Format recent conversation history"""
    if not history:
        return ""
    
    recent_history = history[-max_entries:]
    formatted = "Previous Discussion:\n"
    
    for entry in recent_history:
        expert_name = entry.get('expert', 'Unknown Expert')
        expert_response = entry.get('expert_response', '')
        manager_feedback = entry.get('manager_feedback', '')
        
        formatted += f"Expert ({expert_name}): {expert_response}\n"
        if manager_feedback:
            formatted += f"Manager Assessment: {manager_feedback}\n"
    
    return formatted

def create_expert_prompt(expert, conversation_history):
    """Create prompt for expert response"""
    return f"""
    You are an expert with the following characteristics:
    - Name: {expert['name']} (Age: {expert['age']}, Gender: {expert['gender']})
    - Nationality: {expert['nationality']}
    - Expertise: {expert['expertise']}
    - Personality: {expert['personality']}
    - Communication Style: {expert['speaking_style']}
    
    Interaction Parameters:
    - Positivity: {expert['positivity']}
    - Verbosity: {expert['verbosity']}
    - Explanation Enthusiasm: {expert['explanation_enthusiasm']}

    {format_conversation_history(conversation_history)}

    Consider the opinions of other experts and manager feedback.
    Provide your expert opinion within 200 characters, leveraging your expertise and characteristics.
    """

class ConversationContext:
    def __init__(self, max_history: int = 5):
        self.history: deque = deque(maxlen=max_history)
        self.keywords: Set[str] = set()
        self.expert_insights: Dict[str, List[str]] = {}
        self.current_topic: str = ""
        self.discussion_depth: int = 0
        self.last_update: float = time.time()

    def add_response(self, expert_name: str, expert_response: str, keywords: List[str]) -> None:
        """Add a response to the conversation history with validation"""
        try:
            if not all([expert_name, expert_response]):
                raise ValueError("Expert name and response are required")
                
            entry = {
                "expert": expert_name,
                "expert_response": expert_response,  # í‚¤ ì´ë¦„ ë³€ê²½
                "keywords": keywords,
                "timestamp": time.time()
            }
            self.history.append(entry)
            self.keywords.update(keywords)
            
            if expert_name not in self.expert_insights:
                self.expert_insights[expert_name] = []
            self.expert_insights[expert_name].append(expert_response)
            
            self.discussion_depth += 1
            self.last_update = time.time()
            
            logging.info(f"Added response from {expert_name} with {len(keywords)} keywords")
            
        except Exception as e:
            logging.error(f"Error adding response: {str(e)}")
            raise


class APIRateLimiter:
    def __init__(self, max_requests: int = API_RATE_LIMIT, time_window: int = API_TIME_WINDOW):
        self.requests: deque = deque()
        self.max_requests = max_requests
        self.time_window = time_window
        self.last_retry_time: float = 0
    
    def can_make_request(self) -> bool:
        """Check if a request can be made with cleanup"""
        current_time = time.time()
        
        # Clean up old requests
        while self.requests and current_time - self.requests[0] > self.time_window:
            self.requests.popleft()
            
        # Check if we're within rate limits
        return len(self.requests) < self.max_requests
    
    def add_request(self) -> None:
        """Record a new request"""
        self.requests.append(time.time())
        
    def wait_if_needed(self) -> None:
        """Wait if rate limit is reached"""
        if not self.can_make_request():
            wait_time = self.time_window - (time.time() - self.requests[0])
            if wait_time > 0:
                logging.info(f"Rate limit reached. Waiting {wait_time:.2f} seconds")
                time.sleep(wait_time)

def extract_keywords(text: str, client: genai.GenerativeModel) -> List[str]:
    """Extract keywords using Gemini"""
    try:
        keyword_prompt = f"""
        Extract the most important technical or domain-specific keywords from the following text.
        Return only the keywords as a comma-separated list, without explanations or additional text.
        Focus on meaningful terms and concepts, avoiding common words.
        
        Text: {text}
        
        Keywords:"""
        
        response = client.generate_content(keyword_prompt)
        
        # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ë¦¬
        keywords_text = response.text.strip()
        keywords = [k.strip() for k in keywords_text.split(',') if k.strip()]
        
        logging.info(f"Extracted {len(keywords)} keywords: {', '.join(keywords)}")
        return keywords
        
    except Exception as e:
        logging.error(f"Error in keyword extraction: {str(e)}")
        return []

def get_expert_response(
    client: genai.GenerativeModel,  # íƒ€ì… íŒíŠ¸ ë³€ê²½
    expert: Dict,
    user_todo: str,
    context: ConversationContext,
    rate_limiter: APIRateLimiter
) -> Tuple[str, List[str]]:
    """Get expert response with rate limiting and retries"""
    try:
        rate_limiter.wait_if_needed()
        
        # Build the system prompt with expert's characteristics
        system_prompt = f"""You are {expert['name']}, an expert with the following characteristics:
- Role: {expert['role']}
- Expertise: {expert['expertise']}
- Nationality: {expert['nationality']}
- Personality: {expert['personality']}
- Speaking Style: {expert['speaking_style']}
- Cultural Perspective: {expert['cultural_bias']}

Please analyze the given task from your unique perspective and expertise.
"""

        # Combine system and user prompts for Gemini
        combined_prompt = f"{system_prompt}\n\n{user_todo}"
        
        generation_config = {
            'temperature': expert.get('temperature', 0.7),
            'top_p': 0.8,
            'top_k': 40,
            'max_output_tokens': 2048,
            'candidate_count': 1,
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                rate_limiter.add_request()
                response = client.generate_content(
                    combined_prompt,
                    generation_config=generation_config
                )
                
                expert_response = response.text.strip()
                # Extract keywords using the same model
                keywords = extract_keywords(expert_response, client)
                
                logging.info(f"Expert {expert['name']} response generated successfully")
                return expert_response, keywords
                
            except Exception as e:
                if attempt == MAX_RETRIES - 1:
                    raise
                logging.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                time.sleep(RETRY_DELAY * (attempt + 1))
                
    except Exception as e:
        logging.error(f"Error in expert response: {str(e)}")
        raise


def format_previous_insights(context: ConversationContext) -> str:
    """Format previous expert insights for context"""
    insights = []
    for entry in list(context.history)[-3:]:  # Include the last 3 responses
        insights.append(f"{entry['expert']}: {entry['expert_response']}")
    return "\n\n".join(insights) if insights else "No previous insights available."

def format_manager_feedback(context: ConversationContext) -> str:
    """Format manager feedback for context"""
    feedbacks = []
    for entry in list(context.history)[-3:]:
        if entry.get('manager_feedback'):
            feedbacks.append(f"Feedback on {entry['expert']}:\n{entry['manager_feedback']}")
    return "\n\n".join(feedbacks) if feedbacks else "No manager feedback available."


def get_manager_response(client: genai.GenerativeModel, expert: Dict, expert_response: str, context: ConversationContext):
    """Enhanced manager evaluation with context awareness"""
    try:
        manager_eval_prompt = f"""
        Evaluate {expert['name']}'s contribution in the context of the ongoing discussion:
        
        Discussion Progress: Round {context.discussion_depth}/5
        Current Keywords: {', '.join(sorted(context.keywords))}
        
        Latest Response: {expert_response}
        
        Previous Insights:
        {format_previous_insights(context)}
        
        Provide evaluation in this format:
        âœ“ New Insights:
        - Focus on unique contributions
        
        âš ï¸ Development Needs:
        - Identify gaps in analysis
        
        âœ Discussion Direction:
        - Suggest next focus areas
        
        ğŸ’¡ Cross-Expert Prompts:
        - Specific questions for other experts
        """
        
        # Gemini ìƒì„± ì„¤ì •
        generation_config = {
            'temperature': 0.7,
            'top_p': 0.8,
            'top_k': 40,
            'max_output_tokens': 2048,
            'candidate_count': 1,
        }
        
        # Gemini API í˜¸ì¶œ
        response = client.generate_content(
            f"""You are a strategic discussion facilitator focused on driving deeper insights.
            
            {manager_eval_prompt}""",
            generation_config=generation_config
        )
        
        return response.text.strip()
        
    except Exception as e:
        logging.error(f"Manager evaluation error: {str(e)}")
        raise

def validate_expert_data(expert):
    """ì „ë¬¸ê°€ ë°ì´í„°ì˜ í•„ìˆ˜ í•­ëª©ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜"""
    required_fields = {
        'name': str,
        'role': str,
        'expertise': str,
        'age': int,
        'gender': str,
        'nationality': str,
        'personality': str,
        'speaking_style': str,
        'cultural_bias': str,
        'positivity': float,
        'verbosity': float,
        'explanation_enthusiasm': float,
        'temperature': float
    }
    
    for field, field_type in required_fields.items():
        if field not in expert:
            return False, f"í•„ìˆ˜ í•­ëª© '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        if not isinstance(expert[field], field_type):
            return False, f"'{field}' í•­ëª©ì˜ ë°ì´í„° íƒ€ì…ì´ ì˜ëª»ë˜ìŠµë‹ˆë‹¤."
    return True, ""

def create_expert_team_prompt(user_todo: str, user_profile: Optional[Dict] = None) -> str:
    """ì „ë¬¸ê°€ íŒ€ êµ¬ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    # ê¸°ë³¸ í”„ë¡œí•„ ì„¤ì •
    default_profile = {
        "expertise_level": "intermediate",
        "interest_areas": [],
        "complexity_preference": 0.7
    }
    
    profile = user_profile if user_profile else default_profile
    
    prompt = f"""
    ì£¼ì–´ì§„ ì‘ì—…ê³¼ ì‚¬ìš©ì í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì „ë¬¸ê°€ íŒ€ì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.
    
    ì‘ì—… ë‚´ìš©: {user_todo}
    
    ì‚¬ìš©ì í”„ë¡œí•„:
    - ì „ë¬¸ì„± ìˆ˜ì¤€: {profile['expertise_level']}
    - ê´€ì‹¬ ë¶„ì•¼: {', '.join(profile['interest_areas']) if profile['interest_areas'] else 'ë¯¸ì§€ì •'}
    - ì„ í˜¸ ë³µì¡ë„: {profile['complexity_preference']}
    
    ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    {{
        "experts": [
            {{
                "name": "ì „ë¬¸ê°€ ì´ë¦„",
                "role": "ì „ë¬¸ê°€ ì—­í• ",
                "expertise": "ì „ë¬¸ ë¶„ì•¼",
                "age": ë‚˜ì´(ì •ìˆ˜),
                "gender": "ì„±ë³„",
                "nationality": "êµ­ì ",
                "personality": "ì„±ê²© íŠ¹ì„±",
                "speaking_style": "ëŒ€í™” ìŠ¤íƒ€ì¼",
                "cultural_bias": "ë¬¸í™”ì  ê´€ì ",
                "positivity": ê¸ì •ì„±(0.0~1.0),
                "verbosity": ìƒì„¸ë„(0.0~1.0),
                "explanation_enthusiasm": ì„¤ëª…ì—´ì˜(0.0~1.0),
                "temperature": ì°½ì˜ì„±(0.0~1.0)
            }}
        ]
    }}
    
    ìš”êµ¬ì‚¬í•­:
    1. ê° ì „ë¬¸ê°€ëŠ” ê³ ìœ í•œ ì „ë¬¸ì„±ê³¼ ê´€ì ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
    2. 3-5ëª…ì˜ ì „ë¬¸ê°€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
    3. ë‹¤ì–‘í•œ êµ­ì ê³¼ ë°°ê²½ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”
    4. ëª¨ë“  í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤
    5. ìˆ˜ì¹˜í˜• ë°ì´í„°ëŠ” ì ì ˆí•œ ë²”ìœ„ ë‚´ì—ì„œ ì§€ì •í•´ì£¼ì„¸ìš”
    """
    
    return prompt

def create_expert_team(client: genai.GenerativeModel, user_todo, user_profile=None, max_retries=3):
    """ì „ë¬¸ê°€ íŒ€ êµ¬ì„± í•¨ìˆ˜"""
    for attempt in range(max_retries):
        try:
            expert_team_prompt = create_expert_team_prompt(user_todo, user_profile)
            
            # ë¡œê¹… ì¶”ê°€
            logging.info(f"ì „ë¬¸ê°€ íŒ€ ìƒì„± í”„ë¡¬í”„íŠ¸ (ì‹œë„ {attempt + 1}):\n{expert_team_prompt}")
            
            # Gemini ìƒì„± ì„¤ì •
            generation_config = {
                'temperature': 0.7,
                'top_p': 0.8,
                'top_k': 40,
                'max_output_tokens': 2048,
            }
            
            # Gemini API í˜¸ì¶œ - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            full_prompt = f"""You are a project manager who responds in precise JSON format.

{expert_team_prompt}"""
            
            response = client.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = response.text.strip()
            
            # ì‘ë‹µ ë¡œê¹…
            logging.info(f"LLM ì‘ë‹µ (ì‹œë„ {attempt + 1}):\n{response_text}")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logging.warning(f"JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                continue
                
            json_content = response_text[json_start:json_end]
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                experts_config = json.loads(json_content)
            except json.JSONDecodeError as e:
                logging.error(f"JSON íŒŒì‹± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}\nì‘ë‹µ: {json_content}")
                continue
            
            if 'experts' not in experts_config:
                logging.warning(f"experts í‚¤ê°€ ì—†ìŒ (ì‹œë„ {attempt + 1})")
                continue
            
            # ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦
            experts = experts_config['experts']
            all_valid = True
            error_message = ""
            
            for expert in experts:
                is_valid, error = validate_expert_data(expert)
                if not is_valid:
                    all_valid = False
                    error_message = error
                    logging.error(f"ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error}")
                    break
            
            if all_valid:
                logging.info("ì „ë¬¸ê°€ íŒ€ ìƒì„± ì„±ê³µ")
                return True, experts
            else:
                if attempt == max_retries - 1:
                    return False, f"ì „ë¬¸ê°€ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {error_message}"
                continue
                
        except Exception as e:
            logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}")
            if attempt == max_retries - 1:
                return False, f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            continue
    
    return False, "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."


def estimate_tokens(text):
    """Estimate token count (roughly 4 chars per token for English)"""
    return len(text) // 4

def check_token_limit(prompt, history, limit=7000):
    """Check token usage"""
    total_tokens = estimate_tokens(prompt)
    for entry in history:
        total_tokens += estimate_tokens(str(entry))
    return total_tokens < limit, total_tokens

def update_user_profile(current_profile, new_keywords, client: genai.GenerativeModel):
    """Update user profile based on keywords"""
    try:
        current_keywords = current_profile.get("keywords", [])
        updated_keywords = list(dict.fromkeys(current_keywords + new_keywords))[-50:]
        
        analysis_prompt = f"""
        Analyze the user profile based on these keywords and their frequencies:
        {json.dumps(dict(Counter(updated_keywords)), ensure_ascii=False)}

        Current Profile:
        {json.dumps(current_profile, ensure_ascii=False)}

        Respond in JSON format:
        {{
            "expertise_level": "beginner/intermediate/expert",
            "interest_areas": ["primary interest 1", "primary interest 2", ...],
            "complexity_preference": 0.1~1.0,
            "progression_note": "brief note about user's growth/changes"
        }}
        """
        
        # Gemini ìƒì„± ì„¤ì •
        generation_config = {
            'temperature': 0.3,
            'top_p': 0.8,
            'top_k': 40,
            'max_output_tokens': 2048,
            'candidate_count': 1,
        }
        
        # Gemini API í˜¸ì¶œ
        response = client.generate_content(
            f"""You are a user learning pattern analysis expert.
            
            {analysis_prompt}""",
            generation_config=generation_config
        )
        
        updated_analysis = json.loads(response.text)
        
        # í”„ë¡œí•„ ì—…ë°ì´íŠ¸
        new_profile = {
            "expertise_level": updated_analysis["expertise_level"],
            "keywords": updated_keywords,
            "interest_areas": updated_analysis["interest_areas"],
            "complexity_preference": updated_analysis["complexity_preference"],
            "last_update": datetime.now().isoformat(),
            "progression_note": updated_analysis.get("progression_note", "")
        }
        
        logging.info(f"í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì™„ë£Œ:\n{json.dumps(new_profile, ensure_ascii=False, indent=2)}")
        return new_profile
        
    except Exception as e:
        logging.error(f"í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return current_profile

def get_profile_directory():
    """í”„ë¡œí•„ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ë°˜í™˜"""
    # ì‚¬ìš©ìì˜ í™ˆ ë””ë ‰í† ë¦¬ ì•„ë˜ì— .experts_llm ë””ë ‰í† ë¦¬ ìƒì„±
    profile_dir = Path.home() / '.experts_llm' / 'profiles'
    profile_dir.mkdir(parents=True, exist_ok=True)
    return profile_dir

def save_user_profile(profile_data: Dict) -> None:
    """Save user profile to JSON file"""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì— profiles í´ë” ìƒì„±
        current_dir = Path(__file__).parent
        profile_dir = current_dir / 'profiles'
        profile_dir.mkdir(exist_ok=True)
        
        profile_path = profile_dir / 'user_profile.json'
        
        # ê¸°ì¡´ í”„ë¡œí•„ì´ ìˆë‹¤ë©´ ì½ì–´ì˜¤ê¸°
        if profile_path.exists():
            with open(profile_path, 'r', encoding='utf-8') as f:
                existing_profile = json.load(f)
        else:
            existing_profile = {
                "expertise_level": "intermediate",
                "keywords": [],
                "interest_areas": [],
                "complexity_preference": 0.7
            }
        
        # í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸
        existing_profile["keywords"] = list(set(existing_profile.get("keywords", []) + profile_data.get("keywords", [])))
        
        # í”„ë¡œí•„ ì €ì¥
        with open(profile_path, 'w', encoding='utf-8') as f:
            json.dump(existing_profile, f, ensure_ascii=False, indent=2)
            
        logging.info(f"User profile saved: {profile_path}")
        
    except Exception as e:
        logging.error(f"Error saving user profile: {str(e)}")



def load_user_profile() -> Dict:
    """Load user profile from JSON file"""
    try:
        current_dir = Path(__file__).parent
        profile_path = current_dir / 'profiles' / 'user_profile.json'
        
        if profile_path.exists():
            with open(profile_path, 'r', encoding='utf-8') as f:
                profile = json.load(f)
                logging.info(f"í”„ë¡œí•„ ë¡œë“œë¨: {profile_path}")
                return profile
    except Exception as e:
        logging.error(f"Error loading user profile: {str(e)}")
    
    # ê¸°ë³¸ í”„ë¡œí•„ ë°˜í™˜
    return {
        "expertise_level": "intermediate",
        "keywords": [],
        "interest_areas": [],
        "complexity_preference": 0.7
    }

def show_profile_info():
    """í”„ë¡œí•„ ì •ë³´ë¥¼ ì‚¬ì´ë“œë°”ì— í‘œì‹œ"""
    try:
        profile_dir = get_profile_directory()
        current_profile = load_user_profile()
        
        st.sidebar.subheader("ğŸ“Š í”„ë¡œí•„ ì •ë³´")
        
        if current_profile:
            st.sidebar.markdown("**í˜„ì¬ í”„ë¡œí•„:**")
            st.sidebar.json(current_profile)
            
            # ë°±ì—… íŒŒì¼ ëª©ë¡
            backup_files = sorted(profile_dir.glob('user_profile_backup_*.json'))
            if backup_files:
                st.sidebar.markdown("**ë°±ì—… ê¸°ë¡:**")
                for backup in backup_files[-3:]:  # ìµœê·¼ 3ê°œë§Œ í‘œì‹œ
                    timestamp = backup.stem.split('_')[-1]
                    st.sidebar.text(f"â€¢ ë°±ì—…: {timestamp}")
        else:
            st.sidebar.warning("ì €ì¥ëœ í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.sidebar.markdown(f"**ì €ì¥ ìœ„ì¹˜:** `{profile_dir}`")
        
    except Exception as e:
        st.sidebar.error(f"í”„ë¡œí•„ ì •ë³´ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def get_custom_css() -> str:
    """Get custom CSS for styling"""
    return """
    <style>
        .expert-response {
            margin: 10px 0;
            padding: 15px;
            border-radius: 5px;
            width: 80%;
            font-size: 0.95em;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .manager-response {
            margin-left: 10%;  /* 20% -> 10% ë¡œ ë³€ê²½ */
            padding: 12px;
            border-radius: 5px;
            width: 80%;        /* 20% -> 80% ë¡œ ë³€ê²½ */
            font-size: 0.9em;  /* 0.7em -> 0.9em ìœ¼ë¡œ ë³€ê²½ */
            background-color: #f5f5f5;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .discussion-progress {
            position: fixed;
            top: 10px;
            right: 10px;
            padding: 8px;
            background-color: #f0f8ff;
            border-radius: 4px;
            font-size: 0.8em;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .expert-1 { font-family: 'Arial', sans-serif; }
        .expert-2 { font-family: 'Georgia', serif; }
        .expert-3 { font-family: 'Verdana', sans-serif; }
        .expert-4 { font-family: 'Trebuchet MS', sans-serif; }
        .expert-5 { font-family: 'Palatino', serif; }
        
        .keywords-display {
            margin: 10px 0;
            padding: 10px;
            background-color: #f0f8ff;
            border-radius: 5px;
            font-size: 0.9em;
        }
    </style>
    """

def reset_conversation():
    """ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”"""
    st.session_state.current_round = 0
    st.session_state.conversation_history = []
    st.session_state.expert_data = []
    # í‚¤ì›Œë“œëŠ” ìœ ì§€í•˜ë˜, í˜„ì¬ ì„¸ì…˜ í‚¤ì›Œë“œë§Œ ì´ˆê¸°í™”
    st.session_state.session_keywords = set()

def process_expert_analysis(client: genai.GenerativeModel, user_todo: str) -> None:  # Groq -> GenerativeModelë¡œ ë³€ê²½
    """Process expert analysis with progress tracking and error handling"""
    try:
        with st.spinner('Composing expert team...'):
            success, expert_team = create_expert_team(client, user_todo)
            if not success:
                st.error(f"Failed to create expert team: {expert_team}")
                return
                
            st.session_state.expert_data = expert_team
            logging.info(f"Expert team created with {len(expert_team)} members")
            
        # Create conversation container
        conversation_container = st.container()
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Reset session keywords for new conversation
        st.session_state.session_keywords = set()
        st.session_state.conversation_history = []
        
        # Process each expert's response
        for idx, expert in enumerate(expert_team):
            try:
                progress = (idx + 1) / len(expert_team)
                progress_bar.progress(progress)
                status_text.text(f'Processing expert {idx + 1}/{len(expert_team)}...')
                
                # Get expert response
                expert_response, keywords = get_expert_response(
                    client,
                    expert,
                    user_todo,
                    st.session_state.conversation_context,
                    st.session_state.rate_limiter
                )
                
                # Update session keywords and save profile
                if keywords:
                    st.session_state.session_keywords.update(keywords)
                    # ê° ì „ë¬¸ê°€ ì‘ë‹µ í›„ í”„ë¡œí•„ ì—…ë°ì´íŠ¸
                    profile_update = {
                        "keywords": list(st.session_state.session_keywords)
                    }
                    save_user_profile(profile_update)
                    logging.info(f"Profile updated with keywords from {expert['name']}")
                
                # Display expert response
                with conversation_container:
                    st.markdown(f"""
                        <div class="expert-response expert-{idx + 1}">
                            <strong>{expert['name']}</strong>: {expert_response}
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Get and display manager response
                    manager_response = get_manager_response(
                        client,
                        expert,
                        expert_response,
                        st.session_state.conversation_context
                    )
                    
                    st.markdown(f"""
                        <div class="manager-response">
                            <strong>Manager</strong>: {manager_response}
                        </div>
                    """, unsafe_allow_html=True)
                
                # Save conversation entry
                conversation_entry = {
                    "expert": expert['name'],
                    "expert_response": expert_response,
                    "manager_feedback": manager_response,
                    "keywords": list(keywords),
                    "timestamp": datetime.now().isoformat()
                }
                st.session_state.conversation_history.append(conversation_entry)

                # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                st.session_state.conversation_context.add_response(expert['name'], expert_response, keywords)
                
                # Update sidebar with current keywords and profile info
                with st.sidebar:
                    st.subheader("ğŸ”‘ Current Session Keywords")
                    st.markdown(f"""
                        <div class="keywords-display">
                            {', '.join(sorted(st.session_state.session_keywords))}
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Show profile update status
                    st.success(f"Profile updated with {len(keywords)} new keywords")
                
            except Exception as e:
                error_msg = f"Error processing expert {expert['name']}: {str(e)}"
                logging.error(error_msg)
                st.error(error_msg)
                continue
        
        # Complete progress
        progress_bar.progress(1.0)
        status_text.text('Analysis completed! Generating summary...')

        # ëŒ€í™” ìš”ì•½ ìƒì„± ë° í‘œì‹œ
        with st.spinner('Generating conversation summary...'):
            summary = summarize_conversation(client, st.session_state.conversation_history)
            
            # ìš”ì•½ í‘œì‹œ
            st.markdown("## ğŸ’« Discussion Summary")
            st.markdown(summary)
            
            # ìš”ì•½ì„ íŒŒì¼ë¡œ ì €ì¥
            try:
                summary_path = save_conversation_summary(summary, user_todo)
                st.success(f"Summary saved to: {summary_path}")
            except Exception as e:
                st.warning(f"Failed to save summary: {str(e)}")
            
            # ìš”ì•½ì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
            summary_entry = {
                "type": "summary",
                "content": summary,
                "file_path": summary_path,  # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
                "timestamp": datetime.now().isoformat()
            }
            st.session_state.conversation_history.append(summary_entry)
            
            # ë¡œê¹…
            logging.info("Conversation summary added to history and saved to file")

        status_text.text('Analysis and summary completed!')
        
    except Exception as e:
        error_msg = f"Error in expert analysis: {str(e)}"
        logging.error(error_msg)
        st.error(error_msg)



def estimate_token_usage(context: ConversationContext) -> int:
    """Estimate token usage from conversation context"""
    total_tokens = 0
    for entry in context.history:
        # Rough estimate: 1 token per 4 characters
        total_tokens += len(str(entry)) // 4
    return total_tokens

def reset_conversation_state() -> None:
    """Reset conversation state for new analysis"""
    st.session_state.conversation_context = ConversationContext()
    st.session_state.expert_data = []
    st.session_state.session_keywords = set()
    st.session_state.conversation_history = []
    logging.info("Conversation state reset")

def main():
    if not setup_logging():
        st.error("Failed to initialize logging system")
        return
        
    try:
        st.markdown(get_custom_css(), unsafe_allow_html=True)
        initialize_session_state()
        
        st.title("Expert LLM System")
        
        # API í‚¤ ì´ˆê¸°í™” ë° ê²€ì¦
        if not initialize_api_key():
            st.warning("Please configure your Gemini API key in the sidebar to continue.")
            return
            
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        genai.configure(api_key=st.session_state.api_key)
        client = genai.GenerativeModel('gemini-1.5-pro-latest')
        logging.info("API client initialized successfully")
        
        # User input
        user_todo = st.text_area(
            "Enter your task:",
            placeholder="Example: Analyze the impact of AI on healthcare",
            help="Be specific about what you want to analyze"
        )
        
        if st.button("Start Expert Analysis"):
            if not user_todo:
                st.warning("Please enter a task to analyze")
                return
            process_expert_analysis(client, user_todo)
            
    except Exception as e:
        error_msg = f"Application error: {str(e)}"
        logging.error(error_msg)
        st.error(error_msg)

if __name__ == "__main__":
    main()
